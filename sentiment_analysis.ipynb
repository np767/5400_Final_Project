{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3c58ec1",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "230b46e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import pipeline, AutoTokenizer, AutoModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4336c1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "/opt/anaconda3/envs/dsan_lab/lib/python3.13/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"text-classification\",model='bhadresh-savani/distilbert-base-uncased-emotion', return_all_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "815636a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>politician</th>\n",
       "      <th>type_of_speech</th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>king_angus_s_jr</td>\n",
       "      <td>bipartisan_and_other_speeches</td>\n",
       "      <td>bipartisan_unknown_senators_king_collins_celeb...</td>\n",
       "      <td>skip to content click here to sign up for the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>king_angus_s_jr</td>\n",
       "      <td>bipartisan_and_other_speeches</td>\n",
       "      <td>bipartisan_unknown_king_questions_experts_on_i...</td>\n",
       "      <td>skip to content click here to sign up for the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>king_angus_s_jr</td>\n",
       "      <td>bipartisan_and_other_speeches</td>\n",
       "      <td>bipartisan_unknown_king_backed_legislation_wou...</td>\n",
       "      <td>skip to content click here to sign up for the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>king_angus_s_jr</td>\n",
       "      <td>bipartisan_and_other_speeches</td>\n",
       "      <td>bipartisan_unknown_king_colleagues_call_on_maj...</td>\n",
       "      <td>skip to content click here to sign up for the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>king_angus_s_jr</td>\n",
       "      <td>bipartisan_and_other_speeches</td>\n",
       "      <td>bipartisan_unknown_king_questions_strategic_co...</td>\n",
       "      <td>skip to content click here to sign up for the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023</th>\n",
       "      <td>biggs_andy</td>\n",
       "      <td>bipartisan_and_other_speeches</td>\n",
       "      <td>bipartisan_unknown_congressman_biggs_urges_doj...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>biggs_andy</td>\n",
       "      <td>bipartisan_and_other_speeches</td>\n",
       "      <td>bipartisan_unknown_congressman_biggs_dhs_must_...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025</th>\n",
       "      <td>biggs_andy</td>\n",
       "      <td>bipartisan_and_other_speeches</td>\n",
       "      <td>bipartisan_unknown_press_releases_congressman_...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026</th>\n",
       "      <td>biggs_andy</td>\n",
       "      <td>bipartisan_and_other_speeches</td>\n",
       "      <td>bipartisan_unknown_congressman_biggs_applauds_...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2027</th>\n",
       "      <td>biggs_andy</td>\n",
       "      <td>bipartisan_and_other_speeches</td>\n",
       "      <td>bipartisan_unknown_congressman_biggs_introduce...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2028 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           politician                 type_of_speech  \\\n",
       "0     king_angus_s_jr  bipartisan_and_other_speeches   \n",
       "1     king_angus_s_jr  bipartisan_and_other_speeches   \n",
       "2     king_angus_s_jr  bipartisan_and_other_speeches   \n",
       "3     king_angus_s_jr  bipartisan_and_other_speeches   \n",
       "4     king_angus_s_jr  bipartisan_and_other_speeches   \n",
       "...               ...                            ...   \n",
       "2023       biggs_andy  bipartisan_and_other_speeches   \n",
       "2024       biggs_andy  bipartisan_and_other_speeches   \n",
       "2025       biggs_andy  bipartisan_and_other_speeches   \n",
       "2026       biggs_andy  bipartisan_and_other_speeches   \n",
       "2027       biggs_andy  bipartisan_and_other_speeches   \n",
       "\n",
       "                                              file_name  \\\n",
       "0     bipartisan_unknown_senators_king_collins_celeb...   \n",
       "1     bipartisan_unknown_king_questions_experts_on_i...   \n",
       "2     bipartisan_unknown_king_backed_legislation_wou...   \n",
       "3     bipartisan_unknown_king_colleagues_call_on_maj...   \n",
       "4     bipartisan_unknown_king_questions_strategic_co...   \n",
       "...                                                 ...   \n",
       "2023  bipartisan_unknown_congressman_biggs_urges_doj...   \n",
       "2024  bipartisan_unknown_congressman_biggs_dhs_must_...   \n",
       "2025  bipartisan_unknown_press_releases_congressman_...   \n",
       "2026  bipartisan_unknown_congressman_biggs_applauds_...   \n",
       "2027  bipartisan_unknown_congressman_biggs_introduce...   \n",
       "\n",
       "                                                   text  \n",
       "0     skip to content click here to sign up for the ...  \n",
       "1     skip to content click here to sign up for the ...  \n",
       "2     skip to content click here to sign up for the ...  \n",
       "3     skip to content click here to sign up for the ...  \n",
       "4     skip to content click here to sign up for the ...  \n",
       "...                                                 ...  \n",
       "2023                                                     \n",
       "2024                                                     \n",
       "2025                                                     \n",
       "2026                                                     \n",
       "2027                                                     \n",
       "\n",
       "[2028 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = Path(\"data/processed\")\n",
    "politician = []\n",
    "type_of_speech = []\n",
    "file_name= []\n",
    "text = []\n",
    "\n",
    "for file in file_path.rglob(\"*.txt\"):\n",
    "    #separated_file_name = str(file).split(\"\\\\\") # windows\n",
    "\n",
    "    #politician.append(separated_file_name[2])\n",
    "    #type_of_speech.append(separated_file_name[3])\n",
    "    #file_name.append(separated_file_name[4])\n",
    "    \n",
    "    parts = file.parts # Safe for all OS\n",
    "    politician.append(parts[-3])\n",
    "    type_of_speech.append(parts[-2])\n",
    "    file_name.append(parts[-1])\n",
    "    \n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        text.append(content)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'politician': politician,\n",
    "    'type_of_speech': type_of_speech,\n",
    "    'file_name': file_name,\n",
    "    'text': text\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77ce2d0",
   "metadata": {},
   "source": [
    "### Hugging Face Model Limitations\n",
    "\n",
    "Only accepts 512 tokens at a time. . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be12b037",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"bhadresh-savani/distilbert-base-uncased-emotion\"\n",
    ")\n",
    "\n",
    "def chunk_text(text, max_tokens=512):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    # break tokens into lists of <=512\n",
    "    chunked = [tokens[i:i+max_tokens] for i in range(0, len(tokens), max_tokens)]\n",
    "    # decode each chunk back to text\n",
    "    return [tokenizer.decode(chunk) for chunk in chunked]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29666d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_long_text(text, classifier, tokenizer, max_tokens=512):\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        return {label: 0.0 for label in [\"sadness\",\"joy\",\"love\",\"anger\",\"fear\",\"surprise\"]}\n",
    "\n",
    "    # Tokenize text\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "\n",
    "    # Split tokens into chunks <= max_tokens\n",
    "    chunks = [tokens[i:i+max_tokens] for i in range(0, len(tokens), max_tokens)]\n",
    "    \n",
    "    # Decode each chunk and truncate to max_tokens just in case\n",
    "    decoded_chunks = [tokenizer.decode(chunk, skip_special_tokens=True) for chunk in chunks]\n",
    "\n",
    "    # Run classifier with truncation\n",
    "    results = classifier(decoded_chunks, truncation=True, max_length=max_tokens)\n",
    "\n",
    "    # Extract labels\n",
    "    labels = [d[\"label\"] for d in results[0]]\n",
    "    score_matrix = np.array([[d[\"score\"] for d in chunk] for chunk in results])\n",
    "    avg_scores = score_matrix.mean(axis=0)\n",
    "\n",
    "    return dict(zip(labels, avg_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78c9bab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sadness': np.float64(0.023634135723114014),\n",
       " 'joy': np.float64(0.7674825191497803),\n",
       " 'love': np.float64(0.019324984401464462),\n",
       " 'anger': np.float64(0.16767126321792603),\n",
       " 'fear': np.float64(0.019324718043208122),\n",
       " 'surprise': np.float64(0.002562319627031684)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# small batch test\n",
    "classify_long_text(df.loc[0, \"text\"], classifier, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc6277d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (599 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "###WARNING LONG RUNTIME###\n",
    "\n",
    "df[\"sentiment_scores\"] = df[\"text\"].apply(lambda t: classify_long_text(t, classifier, tokenizer))\n",
    "sentiment_df = df[\"sentiment_scores\"].apply(pd.Series)\n",
    "df = pd.concat([df, sentiment_df], axis = 1) #Creating one df with each sentiment score as its own column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e0001eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>politician</th>\n",
       "      <th>type_of_speech</th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>king_angus_s_jr</td>\n",
       "      <td>bipartisan_and_other_speeches</td>\n",
       "      <td>bipartisan_unknown_senators_king_collins_celeb...</td>\n",
       "      <td>skip to content click here to sign up for the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>king_angus_s_jr</td>\n",
       "      <td>bipartisan_and_other_speeches</td>\n",
       "      <td>bipartisan_unknown_king_questions_experts_on_i...</td>\n",
       "      <td>skip to content click here to sign up for the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>king_angus_s_jr</td>\n",
       "      <td>bipartisan_and_other_speeches</td>\n",
       "      <td>bipartisan_unknown_king_backed_legislation_wou...</td>\n",
       "      <td>skip to content click here to sign up for the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>king_angus_s_jr</td>\n",
       "      <td>bipartisan_and_other_speeches</td>\n",
       "      <td>bipartisan_unknown_king_colleagues_call_on_maj...</td>\n",
       "      <td>skip to content click here to sign up for the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>king_angus_s_jr</td>\n",
       "      <td>bipartisan_and_other_speeches</td>\n",
       "      <td>bipartisan_unknown_king_questions_strategic_co...</td>\n",
       "      <td>skip to content click here to sign up for the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        politician                 type_of_speech  \\\n",
       "0  king_angus_s_jr  bipartisan_and_other_speeches   \n",
       "1  king_angus_s_jr  bipartisan_and_other_speeches   \n",
       "2  king_angus_s_jr  bipartisan_and_other_speeches   \n",
       "3  king_angus_s_jr  bipartisan_and_other_speeches   \n",
       "4  king_angus_s_jr  bipartisan_and_other_speeches   \n",
       "\n",
       "                                           file_name  \\\n",
       "0  bipartisan_unknown_senators_king_collins_celeb...   \n",
       "1  bipartisan_unknown_king_questions_experts_on_i...   \n",
       "2  bipartisan_unknown_king_backed_legislation_wou...   \n",
       "3  bipartisan_unknown_king_colleagues_call_on_maj...   \n",
       "4  bipartisan_unknown_king_questions_strategic_co...   \n",
       "\n",
       "                                                text  \n",
       "0  skip to content click here to sign up for the ...  \n",
       "1  skip to content click here to sign up for the ...  \n",
       "2  skip to content click here to sign up for the ...  \n",
       "3  skip to content click here to sign up for the ...  \n",
       "4  skip to content click here to sign up for the ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac9517b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving new df with columns for each sentiment to output_sentiment\n",
    "\n",
    "df.to_csv(\"output_sentiment.csv\", index = False)\n",
    "# df1, df2, df3 = np.array_split(df, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfec4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1.to_csv(\"output1.csv\", index=False)\n",
    "#df2.to_csv(\"output2.csv\", index=False)\n",
    "#df3.to_csv(\"output3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf9f3d2",
   "metadata": {},
   "source": [
    "## Perform Topic Modeling using output_sentiment.csv\n",
    "\n",
    "### Functions to perform: topic modeling, semantic embeddings & clustering, deeper sentiment analysis, and analysis by group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba9176a",
   "metadata": {},
   "source": [
    "Helper function for performing modeling. Returns: topics with top words used, topic distributions, the NMF model used, and the TF-IDF Vectorizer used.\n",
    "Uses tfidf vectorizer for NMF topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58a6a27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df = pd.read_csv(\"output_sentiment.csv\")\n",
    "\n",
    "def perform_nmf_topic_modeling(texts, n_topics = 5, n_top_words = 10):\n",
    "    #Perform NMF topic modeling\n",
    "    if hasattr(texts, 'tolist'):\n",
    "        texts_list = texts.tolist()\n",
    "    else:\n",
    "        texts_list = list(texts)\n",
    "        \n",
    "    texts_cleaned = []\n",
    "    for text in texts_list:\n",
    "        if pd.isna(text) or text is None:\n",
    "            texts_cleaned.append(\"\")\n",
    "        elif isinstance(text, str):\n",
    "            texts_cleaned.append(text)\n",
    "        else:\n",
    "            texts_cleaned.append(str(text))\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(max_features = 1000, stop_words = 'english', max_df = 0.8, min_df = 2)\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts_cleaned)\n",
    "    \n",
    "    nmf = NMF(n_components=n_topics, random_state=42, max_iter = 200)\n",
    "    nmf.fit(tfidf_matrix)\n",
    "    \n",
    "    #Handle both old and new scikit-learn versions\n",
    "    try:\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "    except AttributeError:\n",
    "        feature_names = vectorizer.get_feature_names()\n",
    "    \n",
    "    #get features\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    #get topics\n",
    "    topics = {}\n",
    "    for topic_idx, topic in enumerate(nmf.components_):\n",
    "        top_words_idx = topic.argsort()[-n_top_words:][::-1]\n",
    "        top_words = [feature_names[i] for i in top_words_idx]\n",
    "        topics[f\"Topic_{topic_idx}\"] = top_words\n",
    "        \n",
    "    topic_distributions = nmf.transform(tfidf_matrix)\n",
    "    return topics, topic_distributions, nmf, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61eccda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NMF Topics:\n",
      "Topic_0: people, going, know, think, want, just, thank, right, like, country\n",
      "Topic_1: mr, speaker, house, act, gentleman, time, shall, section, congressional, yield\n",
      "Topic_2: senator, 2025, trump, act, health, washington, senate, federal, care, press\n",
      "Topic_3: mr, president, presiding, record, officer, senate, congressional, clerk, motion, senator\n"
     ]
    }
   ],
   "source": [
    "#Perform topic modeling\n",
    "topics_nmf, topic_dist_nmf, nmf_model, nmf_vectorizer = perform_nmf_topic_modeling(sentiment_df['text'], n_topics=4)\n",
    "\n",
    "print(\"\\nNMF Topics:\")\n",
    "for topic_name, words in topics_nmf.items():\n",
    "    print(f\"{topic_name}: {', '.join(words)}\")\n",
    "\n",
    "# Add dominant topic to dataframe\n",
    "sentiment_df['dominant_topic_lda'] = topic_dist_nmf.argmax(axis=1)\n",
    "for i in range(topic_dist_nmf.shape[1]):\n",
    "    sentiment_df[f'topic_{i}_score'] = topic_dist_nmf[:, i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0851159",
   "metadata": {},
   "source": [
    "Helper function for semantic embeddings and clustering. Returns an array of embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7afc550",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(texts, model_name = 'all-MiniLM-L6-v2', batch_size = 16):\n",
    "    #Get semantic embeddings using sentence-transformers library\n",
    "    \n",
    "    #Clean texts\n",
    "    if hasattr(texts, 'tolist'):\n",
    "        texts_list = texts.tolist()\n",
    "    else:\n",
    "        texts_list = list(texts)\n",
    "        \n",
    "    texts_cleaned = []\n",
    "    for text in texts_list:\n",
    "        if pd.isna(text) or text is None:\n",
    "            texts_cleaned.append(\"\")\n",
    "        elif isinstance(text, str):\n",
    "            texts_cleaned.append(text.strip())\n",
    "        elif isinstance(text, (int, float)):\n",
    "            texts_cleaned.append(str(text))\n",
    "        else:\n",
    "            texts_cleaned.append(str(text))\n",
    "    \n",
    "    model = SentenceTransformer(model_name)\n",
    "    \n",
    "    embeddings = model.encode(texts, batch_size=batch_size, show_progress_bar = True, convert_to_numpy = True)\n",
    "            \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3382e8ec",
   "metadata": {},
   "source": [
    "Helper function for performing k-means clustering. Returns the clusters, and k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7adb546c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get embeddings and perform clustering\n",
    "#embeddings = get_embeddings(sentiment_df['text'].tolist())\n",
    "\n",
    "def perform_clustering(embeddings, n_clusters = 5):\n",
    "    #Perform k-means clusttering on embeddings\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    clusters = kmeans.fit_predict(embeddings)\n",
    "    return clusters, kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd44c87b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1a1a31b8bd9406793221ab3f4bc0a8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/dsan_lab/lib/python3.13/site-packages/sentence_transformers/SentenceTransformer.py:1621\u001b[39m, in \u001b[36mSentenceTransformer.tokenize\u001b[39m\u001b[34m(self, texts, **kwargs)\u001b[39m\n\u001b[32m   1620\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1621\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1622\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/dsan_lab/lib/python3.13/site-packages/sentence_transformers/models/Transformer.py:307\u001b[39m, in \u001b[36mTransformer.tokenize\u001b[39m\u001b[34m(self, texts, padding)\u001b[39m\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m text_tuple \u001b[38;5;129;01min\u001b[39;00m texts:\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m     batch1.append(\u001b[43mtext_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[32m    308\u001b[39m     batch2.append(text_tuple[\u001b[32m1\u001b[39m])\n",
      "\u001b[31mTypeError\u001b[39m: 'float' object is not subscriptable",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#Get embeddings and perform clustering. Add cluster column to df\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m embeddings = \u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentiment_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m clusters, kmeans_model = perform_clustering(embeddings, n_clusters=\u001b[32m5\u001b[39m)\n\u001b[32m      5\u001b[39m sentiment_df[\u001b[33m'\u001b[39m\u001b[33mcluster\u001b[39m\u001b[33m'\u001b[39m] = clusters\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mget_embeddings\u001b[39m\u001b[34m(texts, model_name, batch_size)\u001b[39m\n\u001b[32m     19\u001b[39m         texts_cleaned.append(\u001b[38;5;28mstr\u001b[39m(text))\n\u001b[32m     21\u001b[39m model = SentenceTransformer(model_name)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m embeddings = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_to_numpy\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/dsan_lab/lib/python3.13/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/dsan_lab/lib/python3.13/site-packages/sentence_transformers/SentenceTransformer.py:1062\u001b[39m, in \u001b[36mSentenceTransformer.encode\u001b[39m\u001b[34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, truncate_dim, pool, chunk_size, **kwargs)\u001b[39m\n\u001b[32m   1060\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m start_index \u001b[38;5;129;01min\u001b[39;00m trange(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(sentences), batch_size, desc=\u001b[33m\"\u001b[39m\u001b[33mBatches\u001b[39m\u001b[33m\"\u001b[39m, disable=\u001b[38;5;129;01mnot\u001b[39;00m show_progress_bar):\n\u001b[32m   1061\u001b[39m     sentences_batch = sentences_sorted[start_index : start_index + batch_size]\n\u001b[32m-> \u001b[39m\u001b[32m1062\u001b[39m     features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1063\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.device.type == \u001b[33m\"\u001b[39m\u001b[33mhpu\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1064\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/dsan_lab/lib/python3.13/site-packages/sentence_transformers/SentenceTransformer.py:1623\u001b[39m, in \u001b[36mSentenceTransformer.tokenize\u001b[39m\u001b[34m(self, texts, **kwargs)\u001b[39m\n\u001b[32m   1621\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[\u001b[32m0\u001b[39m].tokenize(texts, **kwargs)\n\u001b[32m   1622\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/dsan_lab/lib/python3.13/site-packages/sentence_transformers/models/Transformer.py:307\u001b[39m, in \u001b[36mTransformer.tokenize\u001b[39m\u001b[34m(self, texts, padding)\u001b[39m\n\u001b[32m    305\u001b[39m batch1, batch2 = [], []\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m text_tuple \u001b[38;5;129;01min\u001b[39;00m texts:\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m     batch1.append(\u001b[43mtext_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[32m    308\u001b[39m     batch2.append(text_tuple[\u001b[32m1\u001b[39m])\n\u001b[32m    309\u001b[39m to_tokenize = [batch1, batch2]\n",
      "\u001b[31mTypeError\u001b[39m: 'float' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "#Get embeddings and perform clustering. Add cluster column to df\n",
    "embeddings = get_embeddings(sentiment_df['text'].tolist())\n",
    "\n",
    "clusters, kmeans_model = perform_clustering(embeddings, n_clusters=5)\n",
    "sentiment_df['cluster'] = clusters\n",
    "\n",
    "#Visualize clusters using t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(df)-1))\n",
    "embeddings_2d = tsne.fit_transform(embeddings)\n",
    "\n",
    "#Add embeddings to df\n",
    "df['tsne_x'] = embeddings_2d[:,0]\n",
    "df['tsne_y'] = embeddings_2d[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de1fa5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           politician                 type_of_speech  \\\n",
      "0     king_angus_s_jr  bipartisan_and_other_speeches   \n",
      "1     king_angus_s_jr  bipartisan_and_other_speeches   \n",
      "2     king_angus_s_jr  bipartisan_and_other_speeches   \n",
      "3     king_angus_s_jr  bipartisan_and_other_speeches   \n",
      "4     king_angus_s_jr  bipartisan_and_other_speeches   \n",
      "...               ...                            ...   \n",
      "2023       biggs_andy  bipartisan_and_other_speeches   \n",
      "2024       biggs_andy  bipartisan_and_other_speeches   \n",
      "2025       biggs_andy  bipartisan_and_other_speeches   \n",
      "2026       biggs_andy  bipartisan_and_other_speeches   \n",
      "2027       biggs_andy  bipartisan_and_other_speeches   \n",
      "\n",
      "                                              file_name  \\\n",
      "0     bipartisan_unknown_senators_king_collins_celeb...   \n",
      "1     bipartisan_unknown_king_questions_experts_on_i...   \n",
      "2     bipartisan_unknown_king_backed_legislation_wou...   \n",
      "3     bipartisan_unknown_king_colleagues_call_on_maj...   \n",
      "4     bipartisan_unknown_king_questions_strategic_co...   \n",
      "...                                                 ...   \n",
      "2023  bipartisan_unknown_congressman_biggs_urges_doj...   \n",
      "2024  bipartisan_unknown_congressman_biggs_dhs_must_...   \n",
      "2025  bipartisan_unknown_press_releases_congressman_...   \n",
      "2026  bipartisan_unknown_congressman_biggs_applauds_...   \n",
      "2027  bipartisan_unknown_congressman_biggs_introduce...   \n",
      "\n",
      "                                                   text  \\\n",
      "0     skip to content click here to sign up for the ...   \n",
      "1     skip to content click here to sign up for the ...   \n",
      "2     skip to content click here to sign up for the ...   \n",
      "3     skip to content click here to sign up for the ...   \n",
      "4     skip to content click here to sign up for the ...   \n",
      "...                                                 ...   \n",
      "2023                                                NaN   \n",
      "2024                                                NaN   \n",
      "2025                                                NaN   \n",
      "2026                                                NaN   \n",
      "2027                                                NaN   \n",
      "\n",
      "                                       sentiment_scores   sadness       joy  \\\n",
      "0     {'sadness': np.float64(0.023634135723114014), ...  0.023634  0.767483   \n",
      "1     {'sadness': np.float64(0.023634135723114014), ...  0.023634  0.767483   \n",
      "2     {'sadness': np.float64(0.023634135723114014), ...  0.023634  0.767483   \n",
      "3     {'sadness': np.float64(0.023634135723114014), ...  0.023634  0.767483   \n",
      "4     {'sadness': np.float64(0.023634135723114014), ...  0.023634  0.767483   \n",
      "...                                                 ...       ...       ...   \n",
      "2023  {'sadness': 0.0, 'joy': 0.0, 'love': 0.0, 'ang...  0.000000  0.000000   \n",
      "2024  {'sadness': 0.0, 'joy': 0.0, 'love': 0.0, 'ang...  0.000000  0.000000   \n",
      "2025  {'sadness': 0.0, 'joy': 0.0, 'love': 0.0, 'ang...  0.000000  0.000000   \n",
      "2026  {'sadness': 0.0, 'joy': 0.0, 'love': 0.0, 'ang...  0.000000  0.000000   \n",
      "2027  {'sadness': 0.0, 'joy': 0.0, 'love': 0.0, 'ang...  0.000000  0.000000   \n",
      "\n",
      "          love     anger      fear  surprise  dominant_topic_lda  \\\n",
      "0     0.019325  0.167671  0.019325  0.002562                   2   \n",
      "1     0.019325  0.167671  0.019325  0.002562                   2   \n",
      "2     0.019325  0.167671  0.019325  0.002562                   2   \n",
      "3     0.019325  0.167671  0.019325  0.002562                   2   \n",
      "4     0.019325  0.167671  0.019325  0.002562                   2   \n",
      "...        ...       ...       ...       ...                 ...   \n",
      "2023  0.000000  0.000000  0.000000  0.000000                   0   \n",
      "2024  0.000000  0.000000  0.000000  0.000000                   0   \n",
      "2025  0.000000  0.000000  0.000000  0.000000                   0   \n",
      "2026  0.000000  0.000000  0.000000  0.000000                   0   \n",
      "2027  0.000000  0.000000  0.000000  0.000000                   0   \n",
      "\n",
      "      topic_0_score  topic_1_score  topic_2_score  topic_3_score  \n",
      "0               0.0       0.001055       0.100073            0.0  \n",
      "1               0.0       0.001055       0.100073            0.0  \n",
      "2               0.0       0.001055       0.100073            0.0  \n",
      "3               0.0       0.001055       0.100073            0.0  \n",
      "4               0.0       0.001055       0.100073            0.0  \n",
      "...             ...            ...            ...            ...  \n",
      "2023            0.0       0.000000       0.000000            0.0  \n",
      "2024            0.0       0.000000       0.000000            0.0  \n",
      "2025            0.0       0.000000       0.000000            0.0  \n",
      "2026            0.0       0.000000       0.000000            0.0  \n",
      "2027            0.0       0.000000       0.000000            0.0  \n",
      "\n",
      "[2028 rows x 16 columns]\n"
     ]
    }
   ],
   "source": [
    "print(sentiment_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsan_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
