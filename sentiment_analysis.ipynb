{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3c58ec1",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "230b46e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import pipeline, AutoTokenizer, AutoModel\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4336c1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "/opt/anaconda3/envs/dsan_lab/lib/python3.13/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"text-classification\",model='bhadresh-savani/distilbert-base-uncased-emotion', return_all_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "815636a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>politician</th>\n",
       "      <th>type_of_speech</th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>king_angus_s_jr</td>\n",
       "      <td>bipartisan_and_other_speeches</td>\n",
       "      <td>bipartisan_unknown_senators_king_collins_celeb...</td>\n",
       "      <td>skip to content click here to sign up for the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>king_angus_s_jr</td>\n",
       "      <td>bipartisan_and_other_speeches</td>\n",
       "      <td>bipartisan_unknown_king_questions_experts_on_i...</td>\n",
       "      <td>skip to content click here to sign up for the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>king_angus_s_jr</td>\n",
       "      <td>bipartisan_and_other_speeches</td>\n",
       "      <td>bipartisan_unknown_king_backed_legislation_wou...</td>\n",
       "      <td>skip to content click here to sign up for the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>king_angus_s_jr</td>\n",
       "      <td>bipartisan_and_other_speeches</td>\n",
       "      <td>bipartisan_unknown_king_colleagues_call_on_maj...</td>\n",
       "      <td>skip to content click here to sign up for the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>king_angus_s_jr</td>\n",
       "      <td>bipartisan_and_other_speeches</td>\n",
       "      <td>bipartisan_unknown_king_questions_strategic_co...</td>\n",
       "      <td>skip to content click here to sign up for the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023</th>\n",
       "      <td>biggs_andy</td>\n",
       "      <td>bipartisan_and_other_speeches</td>\n",
       "      <td>bipartisan_unknown_congressman_biggs_urges_doj...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>biggs_andy</td>\n",
       "      <td>bipartisan_and_other_speeches</td>\n",
       "      <td>bipartisan_unknown_congressman_biggs_dhs_must_...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025</th>\n",
       "      <td>biggs_andy</td>\n",
       "      <td>bipartisan_and_other_speeches</td>\n",
       "      <td>bipartisan_unknown_press_releases_congressman_...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026</th>\n",
       "      <td>biggs_andy</td>\n",
       "      <td>bipartisan_and_other_speeches</td>\n",
       "      <td>bipartisan_unknown_congressman_biggs_applauds_...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2027</th>\n",
       "      <td>biggs_andy</td>\n",
       "      <td>bipartisan_and_other_speeches</td>\n",
       "      <td>bipartisan_unknown_congressman_biggs_introduce...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2028 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           politician                 type_of_speech  \\\n",
       "0     king_angus_s_jr  bipartisan_and_other_speeches   \n",
       "1     king_angus_s_jr  bipartisan_and_other_speeches   \n",
       "2     king_angus_s_jr  bipartisan_and_other_speeches   \n",
       "3     king_angus_s_jr  bipartisan_and_other_speeches   \n",
       "4     king_angus_s_jr  bipartisan_and_other_speeches   \n",
       "...               ...                            ...   \n",
       "2023       biggs_andy  bipartisan_and_other_speeches   \n",
       "2024       biggs_andy  bipartisan_and_other_speeches   \n",
       "2025       biggs_andy  bipartisan_and_other_speeches   \n",
       "2026       biggs_andy  bipartisan_and_other_speeches   \n",
       "2027       biggs_andy  bipartisan_and_other_speeches   \n",
       "\n",
       "                                              file_name  \\\n",
       "0     bipartisan_unknown_senators_king_collins_celeb...   \n",
       "1     bipartisan_unknown_king_questions_experts_on_i...   \n",
       "2     bipartisan_unknown_king_backed_legislation_wou...   \n",
       "3     bipartisan_unknown_king_colleagues_call_on_maj...   \n",
       "4     bipartisan_unknown_king_questions_strategic_co...   \n",
       "...                                                 ...   \n",
       "2023  bipartisan_unknown_congressman_biggs_urges_doj...   \n",
       "2024  bipartisan_unknown_congressman_biggs_dhs_must_...   \n",
       "2025  bipartisan_unknown_press_releases_congressman_...   \n",
       "2026  bipartisan_unknown_congressman_biggs_applauds_...   \n",
       "2027  bipartisan_unknown_congressman_biggs_introduce...   \n",
       "\n",
       "                                                   text  \n",
       "0     skip to content click here to sign up for the ...  \n",
       "1     skip to content click here to sign up for the ...  \n",
       "2     skip to content click here to sign up for the ...  \n",
       "3     skip to content click here to sign up for the ...  \n",
       "4     skip to content click here to sign up for the ...  \n",
       "...                                                 ...  \n",
       "2023                                                     \n",
       "2024                                                     \n",
       "2025                                                     \n",
       "2026                                                     \n",
       "2027                                                     \n",
       "\n",
       "[2028 rows x 4 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = Path(\"data/processed\")\n",
    "politician = []\n",
    "type_of_speech = []\n",
    "file_name= []\n",
    "text = []\n",
    "\n",
    "for file in file_path.rglob(\"*.txt\"):\n",
    "    #separated_file_name = str(file).split(\"\\\\\") # windows\n",
    "\n",
    "    #politician.append(separated_file_name[2])\n",
    "    #type_of_speech.append(separated_file_name[3])\n",
    "    #file_name.append(separated_file_name[4])\n",
    "    \n",
    "    parts = file.parts # Safe for all OS\n",
    "    politician.append(parts[-3])\n",
    "    type_of_speech.append(parts[-2])\n",
    "    file_name.append(parts[-1])\n",
    "    \n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        text.append(content)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'politician': politician,\n",
    "    'type_of_speech': type_of_speech,\n",
    "    'file_name': file_name,\n",
    "    'text': text\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77ce2d0",
   "metadata": {},
   "source": [
    "### Hugging Face Model Limitations\n",
    "\n",
    "Only accepts 512 tokens at a time. . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be12b037",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"bhadresh-savani/distilbert-base-uncased-emotion\"\n",
    ")\n",
    "\n",
    "def chunk_text(text, max_tokens=512):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    # break tokens into lists of <=512\n",
    "    chunked = [tokens[i:i+max_tokens] for i in range(0, len(tokens), max_tokens)]\n",
    "    # decode each chunk back to text\n",
    "    return [tokenizer.decode(chunk) for chunk in chunked]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29666d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_long_text(text, classifier, tokenizer, max_tokens=512):\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        return {label: 0.0 for label in [\"sadness\",\"joy\",\"love\",\"anger\",\"fear\",\"surprise\"]}\n",
    "\n",
    "    # Tokenize text\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "\n",
    "    # Split tokens into chunks <= max_tokens\n",
    "    chunks = [tokens[i:i+max_tokens] for i in range(0, len(tokens), max_tokens)]\n",
    "    \n",
    "    # Decode each chunk and truncate to max_tokens just in case\n",
    "    decoded_chunks = [tokenizer.decode(chunk, skip_special_tokens=True) for chunk in chunks]\n",
    "\n",
    "    # Run classifier with truncation\n",
    "    results = classifier(decoded_chunks, truncation=True, max_length=max_tokens)\n",
    "\n",
    "    # Extract labels\n",
    "    labels = [d[\"label\"] for d in results[0]]\n",
    "    score_matrix = np.array([[d[\"score\"] for d in chunk] for chunk in results])\n",
    "    avg_scores = score_matrix.mean(axis=0)\n",
    "\n",
    "    return dict(zip(labels, avg_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78c9bab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sadness': np.float64(0.023634135723114014),\n",
       " 'joy': np.float64(0.7674825191497803),\n",
       " 'love': np.float64(0.019324984401464462),\n",
       " 'anger': np.float64(0.16767126321792603),\n",
       " 'fear': np.float64(0.019324718043208122),\n",
       " 'surprise': np.float64(0.002562319627031684)}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# small batch test\n",
    "classify_long_text(df.loc[0, \"text\"], classifier, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4dc6277d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sentiment_scores\"] = df[\"text\"].apply(lambda t: classify_long_text(t, classifier, tokenizer))\n",
    "sentiment_df = df[\"sentiment_scores\"].apply(pd.Series)\n",
    "df = pd.concat([df, sentiment_df], axis = 1) #Creating one df with each sentiment score as its own column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e0001eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>politician</th>\n",
       "      <th>type_of_speech</th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment_scores</th>\n",
       "      <th>sadness</th>\n",
       "      <th>joy</th>\n",
       "      <th>love</th>\n",
       "      <th>anger</th>\n",
       "      <th>fear</th>\n",
       "      <th>surprise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>king_angus_s_jr</td>\n",
       "      <td>bipartisan_and_other_speeches</td>\n",
       "      <td>bipartisan_unknown_senators_king_collins_celeb...</td>\n",
       "      <td>skip to content click here to sign up for the ...</td>\n",
       "      <td>{'sadness': 0.023634135723114014, 'joy': 0.767...</td>\n",
       "      <td>0.023634</td>\n",
       "      <td>0.767483</td>\n",
       "      <td>0.019325</td>\n",
       "      <td>0.167671</td>\n",
       "      <td>0.019325</td>\n",
       "      <td>0.002562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>king_angus_s_jr</td>\n",
       "      <td>bipartisan_and_other_speeches</td>\n",
       "      <td>bipartisan_unknown_king_questions_experts_on_i...</td>\n",
       "      <td>skip to content click here to sign up for the ...</td>\n",
       "      <td>{'sadness': 0.023634135723114014, 'joy': 0.767...</td>\n",
       "      <td>0.023634</td>\n",
       "      <td>0.767483</td>\n",
       "      <td>0.019325</td>\n",
       "      <td>0.167671</td>\n",
       "      <td>0.019325</td>\n",
       "      <td>0.002562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>king_angus_s_jr</td>\n",
       "      <td>bipartisan_and_other_speeches</td>\n",
       "      <td>bipartisan_unknown_king_backed_legislation_wou...</td>\n",
       "      <td>skip to content click here to sign up for the ...</td>\n",
       "      <td>{'sadness': 0.023634135723114014, 'joy': 0.767...</td>\n",
       "      <td>0.023634</td>\n",
       "      <td>0.767483</td>\n",
       "      <td>0.019325</td>\n",
       "      <td>0.167671</td>\n",
       "      <td>0.019325</td>\n",
       "      <td>0.002562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>king_angus_s_jr</td>\n",
       "      <td>bipartisan_and_other_speeches</td>\n",
       "      <td>bipartisan_unknown_king_colleagues_call_on_maj...</td>\n",
       "      <td>skip to content click here to sign up for the ...</td>\n",
       "      <td>{'sadness': 0.023634135723114014, 'joy': 0.767...</td>\n",
       "      <td>0.023634</td>\n",
       "      <td>0.767483</td>\n",
       "      <td>0.019325</td>\n",
       "      <td>0.167671</td>\n",
       "      <td>0.019325</td>\n",
       "      <td>0.002562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>king_angus_s_jr</td>\n",
       "      <td>bipartisan_and_other_speeches</td>\n",
       "      <td>bipartisan_unknown_king_questions_strategic_co...</td>\n",
       "      <td>skip to content click here to sign up for the ...</td>\n",
       "      <td>{'sadness': 0.023634135723114014, 'joy': 0.767...</td>\n",
       "      <td>0.023634</td>\n",
       "      <td>0.767483</td>\n",
       "      <td>0.019325</td>\n",
       "      <td>0.167671</td>\n",
       "      <td>0.019325</td>\n",
       "      <td>0.002562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        politician                 type_of_speech  \\\n",
       "0  king_angus_s_jr  bipartisan_and_other_speeches   \n",
       "1  king_angus_s_jr  bipartisan_and_other_speeches   \n",
       "2  king_angus_s_jr  bipartisan_and_other_speeches   \n",
       "3  king_angus_s_jr  bipartisan_and_other_speeches   \n",
       "4  king_angus_s_jr  bipartisan_and_other_speeches   \n",
       "\n",
       "                                           file_name  \\\n",
       "0  bipartisan_unknown_senators_king_collins_celeb...   \n",
       "1  bipartisan_unknown_king_questions_experts_on_i...   \n",
       "2  bipartisan_unknown_king_backed_legislation_wou...   \n",
       "3  bipartisan_unknown_king_colleagues_call_on_maj...   \n",
       "4  bipartisan_unknown_king_questions_strategic_co...   \n",
       "\n",
       "                                                text  \\\n",
       "0  skip to content click here to sign up for the ...   \n",
       "1  skip to content click here to sign up for the ...   \n",
       "2  skip to content click here to sign up for the ...   \n",
       "3  skip to content click here to sign up for the ...   \n",
       "4  skip to content click here to sign up for the ...   \n",
       "\n",
       "                                    sentiment_scores   sadness       joy  \\\n",
       "0  {'sadness': 0.023634135723114014, 'joy': 0.767...  0.023634  0.767483   \n",
       "1  {'sadness': 0.023634135723114014, 'joy': 0.767...  0.023634  0.767483   \n",
       "2  {'sadness': 0.023634135723114014, 'joy': 0.767...  0.023634  0.767483   \n",
       "3  {'sadness': 0.023634135723114014, 'joy': 0.767...  0.023634  0.767483   \n",
       "4  {'sadness': 0.023634135723114014, 'joy': 0.767...  0.023634  0.767483   \n",
       "\n",
       "       love     anger      fear  surprise  \n",
       "0  0.019325  0.167671  0.019325  0.002562  \n",
       "1  0.019325  0.167671  0.019325  0.002562  \n",
       "2  0.019325  0.167671  0.019325  0.002562  \n",
       "3  0.019325  0.167671  0.019325  0.002562  \n",
       "4  0.019325  0.167671  0.019325  0.002562  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eac9517b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving new df with columns for each sentiment to output_sentiment\n",
    "df.to_csv(\"output_sentiment.csv\", index = False)\n",
    "# df1, df2, df3 = np.array_split(df, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfec4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(\"output1.csv\", index=False)\n",
    "df2.to_csv(\"output2.csv\", index=False)\n",
    "df3.to_csv(\"output3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf9f3d2",
   "metadata": {},
   "source": [
    "## Perform Topic Modeling using output_sentiment.csv\n",
    "\n",
    "### Functions to perform: topic modeling, semantic embeddings & clustering, deeper sentiment analysis, and analysis by group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba9176a",
   "metadata": {},
   "source": [
    "Helper function for performing modeling. Returns: topics with top words used, topic distributions, the NMF model used, and the TF-IDF Vectorizer used.\n",
    "Uses tfidf vectorizer for NMF topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "58a6a27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df = pd.read_csv(\"output_sentiment.csv\")\n",
    "\n",
    "def perform_nmf_topic_modeling(texts, n_topics = 5, n_top_words = 10):\n",
    "    #Perform NMF topic modeling\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(max_features = 1000, stop_words = 'english', max_df = 0.8, min_df = 2)\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    \n",
    "    nmf = NMF(n_components=n_topics, random_state=42, max_iter = 200)\n",
    "    nmf.fit(tfidf_matrix)\n",
    "    \n",
    "    #Handle both old and new scikit-learn versions\n",
    "    try:\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "    except AttributeError:\n",
    "        feature_names = vectorizer.get_feature_names()\n",
    "    \n",
    "    #get features\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    #get topics\n",
    "    topics = {}\n",
    "    for topic_idx, topic in enumerate(nmf.components_):\n",
    "        top_words_idx = topic.argsort()[-n_top_words:][::-1]\n",
    "        top_words = [feature_names[i] for i in top_words_idx]\n",
    "        topics[f\"Topic_{topic_idx}\"] = top_words\n",
    "        \n",
    "    topic_distributions = nmf.transform(tfidf_matrix)\n",
    "    return topics, topic_distributions, nmf, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "61eccda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NMF Topics:\n",
      "Topic_0: people, going, know, think, just, want, thank, right, like, country\n",
      "Topic_1: mr, speaker, act, shall, section, time, gentleman, amendment, yield, states\n",
      "Topic_2: senator, 2025, trump, senate, president, washington, senators, administration, act, read\n",
      "Topic_3: mr, record, congressional, house, office, page, speaker, number, president, january\n",
      "Topic_4: health, care, veterans, services, families, legislation, appropriations, insurance, help, committee\n"
     ]
    }
   ],
   "source": [
    "#Perform topic modeling\n",
    "topics_nmf, topic_dist_nmf, nmf_model, nmf_vectorizer = perform_nmf_topic_modeling(df['text'], n_topics=5)\n",
    "\n",
    "print(\"\\nNMF Topics:\")\n",
    "for topic_name, words in topics_nmf.items():\n",
    "    print(f\"{topic_name}: {', '.join(words)}\")\n",
    "\n",
    "# Add dominant topic to dataframe\n",
    "sentiment_df['dominant_topic_lda'] = topic_dist_nmf.argmax(axis=1)\n",
    "for i in range(topic_dist_nmf.shape[1]):\n",
    "    sentiment_df[f'topic_{i}_score'] = topic_dist_nmf[:, i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0851159",
   "metadata": {},
   "source": [
    "Helper function for semantic embeddings and clustering. Returns an array of embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e7afc550",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(texts, model_name = 'sentence-transformers/all-MiniLM-L6-v2'):\n",
    "    #Get semantic embeddings using Hugging Face models\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    \n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        if not isinstance(text, str) or text.strip() == \"\":\n",
    "            embedding_dim = model.config.hidden_size\n",
    "            embeddings.append(np.zeros(embedding_dim))\n",
    "            continue\n",
    "        \n",
    "        #Tokenize and truncate\n",
    "        inputs = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "        \n",
    "        #Get embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            #Use mean pooling\n",
    "            embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "            embeddings.append(embedding)\n",
    "            \n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3382e8ec",
   "metadata": {},
   "source": [
    "Helper function for performing k-means clustering. Returns the clusters, and k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7adb546c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get embeddings and perform clustering\n",
    "#embeddings = get_embeddings(sentiment_df['text'].tolist())\n",
    "\n",
    "def perform_clustering(embeddings, n_clusters = 5):\n",
    "    #Perform k-means clusttering on embeddings\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    clusters = kmeans.fit_predict(embeddings)\n",
    "    return clusters, kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dd44c87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(70551) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#Get embeddings and perform clustering. Add cluster column to df\n",
    "embeddings = get_embeddings(sentiment_df['text'].tolist())\n",
    "\n",
    "clusters, kmeans_model = perform_clustering(embeddings, n_clusters=5)\n",
    "sentiment_df['cluster'] = clusters\n",
    "\n",
    "#Visualize clusters using t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(df)-1))\n",
    "embeddings_2d = tsne.fit_transform(embeddings)\n",
    "\n",
    "#Add embeddings to df\n",
    "df['tsne_x'] = embeddings[:,0]\n",
    "df['tsne_y'] = embeddings[:,1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsan_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
